{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "### : Boosting 기법 중 하나로, 약한 학습기 여러 개를 묶어서 정확도를 높이는 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $Y$ = ${\\alpha}{\\times}M(X)$ + ${\\beta}{\\times}G(X)$ + ${\\gamma}{\\times}H(X)$ + _error_ ($\\alpha, \\beta, \\gamma$는 학습기 $M, G, H$의 각 비중)\n",
    "\n",
    "> ※ 다음 단계의 학습기는 이전 단계의 학습기의 에러를 줄일 수 있도록 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  모델 파라미터 알아보기\n",
    ">> 1. 일반 파라미터(General Parameters)\n",
    "     - booster / nthread / num_feature\n",
    "     \n",
    ">> 2. 부스팅 파라미터(Boosting Parameters)\n",
    "    - eta(learning rate) / gamma(information gain) / max_depth / lamda(L2 reg) / alpha(L1 reg)\n",
    "    - gamma 작을수록, max_depth 클수록, lambda 작을수록, alpha 작을수록 overfitting \n",
    "\n",
    ">> 3. 학습 과정 파라미터(Learning Task Parameters)\n",
    "    - objective function, evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
